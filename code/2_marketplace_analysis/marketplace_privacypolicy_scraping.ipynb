{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Notes\n",
    "1- I started with 1248 mp list\n",
    "\n",
    "2- Extracted the urls of privacy policy from the homepages of the marketplaces and saved in \"marketplace_pp.csv\"\n",
    "\n",
    "3- A lot of urls were not in a good shape so I cleaned then by hand and saved in \"marketplace_pp_clean.csv\"\n",
    "\n",
    "4- Scarepd the pages of privacy policy from the \"marketplace_pp_clean.csv\" list, saved in \"mp_privacy_policies\" and found that only 406 /1248 MPs provided pps.\n",
    "\n",
    "NOTE- doing a manual check if automatic extraction of pp links did not catch links for many websites, if we found any we will have toupdate the results. if 1248-406 = 842 websites donot have the pp , check the as location/country and check what jurisdiction says there about providing a privacy policy, WE DIDNOT FIND ANY.\n",
    "\n",
    "5- Liza cleaned the privacy policies 0f 406 privacy policies\n",
    "\n",
    "6- 18 privacy policies of websites which do not exist anymore or where the certificate was expired were collected through wayback machine \n",
    "\n",
    "7- pp of two market places we could not find on wayback machine eother so we have results for 404/406 pps ( out of 1248 MPs)\n",
    "\n",
    "8- Liza is manually checking the mention of the cookie, third party, tracking, COPPA, etc in 404 websites\n",
    "\n",
    "#Analysis and plots\n",
    "table: show all the languages we had in 404 pp\n",
    "table: random stats\n",
    "heatmap: cosine similarity of the pp text found in 404 websites\n",
    "table: manual checking of the pp text about cookies etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-16T03:48:54.577960Z",
     "start_time": "2022-06-16T03:48:54.160739Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-11T01:03:05.532Z"
    }
   },
   "outputs": [],
   "source": [
    "#get all html\n",
    "MARKETPLACES = []\n",
    "with open(\"marketplaces_hina.csv\") as fin: #list of 1000+ marketplaces filtered by Hina\n",
    "    for line in fin:\n",
    "        MARKETPLACES.append(line.split(',')[0])\n",
    "\n",
    "\n",
    "raw_data = []\n",
    "for mp in MARKETPLACES:\n",
    "    print(\"Retrieving html for\", mp)\n",
    "    try:\n",
    "        response = requests.get(\"http://\" + mp, timeout=10)\n",
    "    except:\n",
    "        print(\"unable to get\", mp)\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pps = soup.find_all(href=re.compile('privacy'))\n",
    "    if pps!=[]:\n",
    "        for pp in pps:\n",
    "            raw_data.append({'website':mp, 'pp':pp['href']})\n",
    "    else:\n",
    "        raw_data.append({'website':mp, 'pp':'NOT_FOUND'})\n",
    "\n",
    "df_pp = pd.DataFrame(raw_data)\n",
    "df_pp.to_csv(\"./marketplace_pp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T00:25:13.864032Z",
     "start_time": "2022-05-26T13:44:31.618192Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\"}\n",
    "\n",
    "df = pd.read_csv('marketplace_pp_clean.csv') # clean means the urls to policies are functional\n",
    "df = df[df['pp_link']!=\"NOT FOUND\"]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        print(\"fetching:\" , row[1])\n",
    "\n",
    "        file_name = row[0]+\"_privacypolicy\"\n",
    "        url = row[1]\n",
    "\n",
    "        path = '/home/hina/iogames/iogames_codebase/marketplace_characterization/mp_privacy_policies/'\n",
    "    #     requests.adapters.DEFAULT_RETRIES = 1\n",
    "        soup = BeautifulSoup(requests.get(url, timeout=10).content, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "\n",
    "        f = open(path+file_name, \"w\")\n",
    "        f.write(text)\n",
    "        f.close() \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-16T06:32:51.078Z"
    }
   },
   "outputs": [],
   "source": [
    "# find all the websites where no policy was given, do an nslookup for all the websites\n",
    "# and find the country of origin, see if you can find jurisdiction about the providing the privacy policy\n",
    "\n",
    "import whois\n",
    "import pycountry\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dff = pd.read_csv(\"marketplace_pp.csv\")\n",
    "mp_no_pp = dff[dff['pp']=='NOT_FOUND']['website'].to_list()\n",
    "L =[]\n",
    "for mp in mp_no_pp:\n",
    "    try:\n",
    "#         print(mp)\n",
    "        ccode = whois.whois(mp)['country']\n",
    "        country = pycountry.countries.get(alpha_2=ccode)\n",
    "        country_name = country.name\n",
    "        d = {'mp':mp, \"country\":country_name}\n",
    "        L.append(d)\n",
    "    except:\n",
    "        d = {'mp':mp, \"country\":'NOT_FOUND'}\n",
    "        L.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-16T06:24:35.926713Z",
     "start_time": "2022-06-16T06:24:35.918778Z"
    }
   },
   "outputs": [],
   "source": [
    "d_f = pd.DataFrame(L).sort_values('country')\n",
    "d_f.to_csv(\"mp_no-privacypolicy_countries.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T01:59:56.881705Z",
     "start_time": "2022-06-17T01:59:56.874072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Afghanistan',\n",
       " 'Austria',\n",
       " 'Bahamas',\n",
       " 'Belarus',\n",
       " 'Brazil',\n",
       " 'Bulgaria',\n",
       " 'Canada',\n",
       " 'Cayman Islands',\n",
       " 'China',\n",
       " 'Croatia',\n",
       " 'Czechia',\n",
       " 'Denmark',\n",
       " 'Egypt',\n",
       " 'France',\n",
       " 'Germany',\n",
       " 'Hungary',\n",
       " 'Iceland',\n",
       " 'India',\n",
       " 'Indonesia',\n",
       " 'Italy',\n",
       " 'Japan',\n",
       " 'Korea, Republic of',\n",
       " 'Latvia',\n",
       " 'Malta',\n",
       " 'NOT_FOUND',\n",
       " 'Netherlands',\n",
       " 'North Macedonia',\n",
       " 'Norway',\n",
       " 'Panama',\n",
       " 'Poland',\n",
       " 'Romania',\n",
       " 'Russian Federation',\n",
       " 'Slovenia',\n",
       " 'Spain',\n",
       " 'Switzerland',\n",
       " 'Turkey',\n",
       " 'Ukraine',\n",
       " 'United Arab Emirates',\n",
       " 'United Kingdom',\n",
       " 'United States',\n",
       " 'Viet Nam'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1248-406 = 842 , adjust 842 - 794 = 48 websites here\n",
    "import pandas as pd\n",
    "d_f = pd.read_csv(\"mp_no-privacypolicy_countries.csv\")\n",
    "set(d_f['country'].to_list())\n",
    "# d_f[d_f['country']=='NOT_FOUND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T01:58:06.133252Z",
     "start_time": "2022-06-17T01:58:05.446855Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'country'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a1d7d1dc120c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwhois\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwhois\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhois\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'www.gamespark.jp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'country'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-26T10:02:21.267192Z",
     "start_time": "2022-05-26T10:02:21.261152Z"
    }
   },
   "outputs": [],
   "source": [
    "# L= []\n",
    "# for index, row in df_read_pp.iterrows():\n",
    "# #     print(row[1])\n",
    "#     if row[2].startswith(\"http\"):\n",
    "#         url = row[2]\n",
    "#         d={\"mp\":row[1],'pp_link':url}\n",
    "#         L.append(d)\n",
    "#     elif row[2].startswith(\"//www\"):\n",
    "#         url = 'http:'+row[2]\n",
    "#         d={\"mp\":row[1],'pp_link':url}\n",
    "#         L.append(d)\n",
    "#     elif row[2].startswith('/'):\n",
    "#         url = \"http://\"+row[1]+row[2]\n",
    "#         d={\"mp\":row[1],'pp_link':url}\n",
    "#         L.append(d)\n",
    "#     elif  row[2].startswith('privacy'):\n",
    "#         url = \"http://\"+row[1]+'/'+row[2]\n",
    "#         d={\"mp\":row[1],'pp_link':url}\n",
    "#         L.append(d)\n",
    "#     else:\n",
    "#         pass\n",
    "# L2 = [\n",
    "# {'mp':  'www.cardgamesolitaire.com' , 'pp_link': 'https://www.cardgamesolitaire.com/privacy-policy.html'},\n",
    "# {'mp':  'www.freegames.ws' , 'pp_link': 'https://www.freegames.ws/freegames_legal/privacy_policy.htm'},\n",
    "# {'mp' : 'freewebsitetemplates.com', 'pp_link': 'https://freewebsitetemplates.com/about/privacy'},\n",
    "# {'mp' : 'www.spiellen.de', 'pp_link': 'https://www.spiellen.de/privacy-policy.php'},\n",
    "# {'mp' : 'www.creagames.com', 'pp_link': 'https://www.creagames.com/privacy'},\n",
    "# {'mp' : 'www.freetypinggame.net', 'pp_link': 'https://www.freetypinggame.net/privacy.asp'},\n",
    "# {'mp' : 'www.gleefulgames.com', 'pp_link': 'http://www.gleefulgames.com/privacy.html'},\n",
    "# {'mp' : 'www.miniclip.com', 'pp_link': 'https://www.miniclip.com/games/page/en/privacy-policy#privacy-settings'},\n",
    "# {'mp' : 'supermariobros.io', 'pp_link': 'NOT FOUND'},\n",
    "# {'mp' : 'www.flyordie.com', 'pp_link': 'https://www.flyordie.com/legal.html#privacy'},\n",
    "# {'mp' : 'www.games1.in', 'pp_link': 'https://www.games1.in/privacy-policy.php'},\n",
    "# {'mp' : 'www.gamegratis33.com', 'pp_link': 'https://www.gamegratis33.com/index.php/privacy'},\n",
    "# {'mp' : 'www.gamku.com', 'pp_link': 'NOT FOUND'},\n",
    "# {'mp' : 'www.jouezgratuitement.fr', 'pp_link': 'https://www.jouezgratuitement.fr/privacy-policy.php'}   \n",
    "# ]\n",
    "\n",
    "# L3 = L+L2\n",
    "# pd.DataFrame(L3).to_csv('marketplace_pp_clean.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-16T03:46:58.472819Z",
     "start_time": "2022-06-16T03:46:58.469655Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T13:06:41.018355Z",
     "start_time": "2022-05-29T13:06:41.014850Z"
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\"}\n",
    "\n",
    "# df = pd.read_csv('marketplace_pp_clean.csv')\n",
    "# df = df[df['pp_link']!=\"NOT FOUND\"]\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     try:\n",
    "#         print(\"fetching:\" , row[1])\n",
    "\n",
    "#         file_name = row[0]+\"_privacypolicy\"\n",
    "#         url = row[1]\n",
    "\n",
    "#         path = '/home/hina/iogames/iogames_codebase/marketplace_characterization/mp_privacy_policies_2/'\n",
    "#     #     requests.adapters.DEFAULT_RETRIES = 1\n",
    "#         soup = BeautifulSoup(requests.get(url, timeout=10).content, \"html.parser\")\n",
    "#         text = soup.get_text()\n",
    "\n",
    "#         f = open(path+file_name, \"w\")\n",
    "#         f.write(text)\n",
    "#         f.close() \n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         pass\n",
    "    \n",
    "#     time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
